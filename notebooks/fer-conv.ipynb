{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition\n",
    "In this approach, we will setup a convolutional neural network to aim for an accuracy > 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from random import randint\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "writer = tf.summary.FileWriter('logs/graph', session.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# images are 48x48 pixels\n",
    "img_size = 48\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "num_classes = 7\n",
    "class_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# probability to drop a unit (prevent overfitting)\n",
    "dropout = 0.25\n",
    "\n",
    "# training parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 20000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Vectors (0-255)\n",
    "imgs_train_byte = []\n",
    "imgs_test_byte = []\n",
    "\n",
    "# Labels (0-6)\n",
    "labels_train_class = []\n",
    "labels_test_class = []\n",
    "\n",
    "\n",
    "with open('../fer2013.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    # skip CSV header\n",
    "    next(readCSV)\n",
    "    for row in readCSV:\n",
    "        if row[2] == 'Training':\n",
    "            # cast pixels to int\n",
    "            pixels_train = [float(x) for x in row[1].split(' ')]\n",
    "            imgs_train_byte.append(np.array(pixels_train))\n",
    "            labels_train_class.append(int(row[0]))\n",
    "        elif row[2] == 'PrivateTest':\n",
    "            pixels_test = [float(x) for x in row[1].split(' ')]\n",
    "            imgs_test_byte.append(np.array(pixels_test))\n",
    "            labels_test_class.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cast to correct type and wrap into np arrays\n",
    "imgs_train_byte = np.array(imgs_train_byte, dtype=np.float32)\n",
    "imgs_test_byte = np.array(imgs_test_byte, dtype=np.float32)\n",
    "labels_train_class = np.array(labels_train_class, dtype=np.float32)\n",
    "labels_test_class = np.array(labels_test_class, dtype=np.float32)\n",
    "# normalize the pixel intensitys\n",
    "imgs_train = np.divide(imgs_train_byte, 255)\n",
    "imgs_test = np.divide(imgs_test_byte, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    \n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "\n",
    "        # The data input is a 1-D vector of 2304 features (48*48 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, img_size, img_size, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "        \n",
    "        # Convolution Layer with 128 filters and a kernel size of 3\n",
    "        conv3 = tf.layers.conv2d(conv2, 128, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv3 = tf.layers.max_pooling2d(conv3, 2, 2)\n",
    "        \n",
    "        # Convolution Layer with 256 filters and a kernel size of 3\n",
    "        conv4 = tf.layers.conv2d(conv3, 256, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv4 = tf.layers.max_pooling2d(conv4, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv4)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model using the Estimator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './model', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Build the Estimator\n",
    "model = tf.estimator.Estimator(model_fn=model_fn, model_dir='../model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.94972, step = 1\n",
      "INFO:tensorflow:global_step/sec: 2.01673\n",
      "INFO:tensorflow:loss = 1.76746, step = 101 (49.586 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.04401\n",
      "INFO:tensorflow:loss = 1.63291, step = 201 (48.923 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.05261\n",
      "INFO:tensorflow:loss = 1.40672, step = 301 (48.718 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.05553\n",
      "INFO:tensorflow:loss = 1.31101, step = 401 (48.649 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.05524\n",
      "INFO:tensorflow:loss = 1.44795, step = 501 (48.656 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.05089\n",
      "INFO:tensorflow:loss = 1.35904, step = 601 (48.760 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.05901\n",
      "INFO:tensorflow:loss = 1.26341, step = 701 (48.567 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.05166\n",
      "INFO:tensorflow:loss = 1.09406, step = 801 (48.741 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.05554\n",
      "INFO:tensorflow:loss = 1.23384, step = 901 (48.649 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.05722\n",
      "INFO:tensorflow:loss = 1.13425, step = 1001 (48.609 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.05414\n",
      "INFO:tensorflow:loss = 1.19439, step = 1101 (48.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.05807\n",
      "INFO:tensorflow:loss = 1.00449, step = 1201 (48.589 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1232 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.05305\n",
      "INFO:tensorflow:loss = 1.13662, step = 1301 (48.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0586\n",
      "INFO:tensorflow:loss = 0.902475, step = 1401 (48.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08189\n",
      "INFO:tensorflow:loss = 0.92142, step = 1501 (48.034 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07927\n",
      "INFO:tensorflow:loss = 0.752641, step = 1601 (48.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08074\n",
      "INFO:tensorflow:loss = 0.994939, step = 1701 (48.060 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07719\n",
      "INFO:tensorflow:loss = 0.813659, step = 1801 (48.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08001\n",
      "INFO:tensorflow:loss = 0.672109, step = 1901 (48.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08256\n",
      "INFO:tensorflow:loss = 0.744711, step = 2001 (48.018 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07927\n",
      "INFO:tensorflow:loss = 0.661998, step = 2101 (48.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07877\n",
      "INFO:tensorflow:loss = 0.653197, step = 2201 (48.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08066\n",
      "INFO:tensorflow:loss = 0.781929, step = 2301 (48.062 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08111\n",
      "INFO:tensorflow:loss = 0.722314, step = 2401 (48.051 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2479 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.07801\n",
      "INFO:tensorflow:loss = 0.469095, step = 2501 (48.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08212\n",
      "INFO:tensorflow:loss = 0.545745, step = 2601 (48.028 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08701\n",
      "INFO:tensorflow:loss = 0.582081, step = 2701 (47.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08091\n",
      "INFO:tensorflow:loss = 0.517002, step = 2801 (48.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08247\n",
      "INFO:tensorflow:loss = 0.476621, step = 2901 (48.020 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08204\n",
      "INFO:tensorflow:loss = 0.653012, step = 3001 (48.030 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08161\n",
      "INFO:tensorflow:loss = 0.51126, step = 3101 (48.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08463\n",
      "INFO:tensorflow:loss = 0.300857, step = 3201 (47.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08222\n",
      "INFO:tensorflow:loss = 0.521607, step = 3301 (48.026 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08355\n",
      "INFO:tensorflow:loss = 0.298844, step = 3401 (47.994 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09214\n",
      "INFO:tensorflow:loss = 0.375397, step = 3501 (47.798 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08099\n",
      "INFO:tensorflow:loss = 0.361595, step = 3601 (48.054 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08301\n",
      "INFO:tensorflow:loss = 0.378036, step = 3701 (48.008 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3729 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.07389\n",
      "INFO:tensorflow:loss = 0.283404, step = 3801 (48.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08615\n",
      "INFO:tensorflow:loss = 0.333185, step = 3901 (47.935 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08821\n",
      "INFO:tensorflow:loss = 0.317801, step = 4001 (47.888 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07864\n",
      "INFO:tensorflow:loss = 0.370648, step = 4101 (48.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08709\n",
      "INFO:tensorflow:loss = 0.327107, step = 4201 (47.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08415\n",
      "INFO:tensorflow:loss = 0.263663, step = 4301 (47.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08109\n",
      "INFO:tensorflow:loss = 0.111525, step = 4401 (48.052 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07961\n",
      "INFO:tensorflow:loss = 0.294605, step = 4501 (48.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07717\n",
      "INFO:tensorflow:loss = 0.209285, step = 4601 (48.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07693\n",
      "INFO:tensorflow:loss = 0.204144, step = 4701 (48.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08006\n",
      "INFO:tensorflow:loss = 0.277082, step = 4801 (48.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08352\n",
      "INFO:tensorflow:loss = 0.153897, step = 4901 (47.996 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4978 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.07476\n",
      "INFO:tensorflow:loss = 0.308775, step = 5001 (48.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07808\n",
      "INFO:tensorflow:loss = 0.164231, step = 5101 (48.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08627\n",
      "INFO:tensorflow:loss = 0.119509, step = 5201 (47.932 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07262\n",
      "INFO:tensorflow:loss = 0.299134, step = 5301 (48.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08497\n",
      "INFO:tensorflow:loss = 0.186713, step = 5401 (47.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0814\n",
      "INFO:tensorflow:loss = 0.11919, step = 5501 (48.044 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07865\n",
      "INFO:tensorflow:loss = 0.187814, step = 5601 (48.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08065\n",
      "INFO:tensorflow:loss = 0.131878, step = 5701 (48.062 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07715\n",
      "INFO:tensorflow:loss = 0.159152, step = 5801 (48.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08849\n",
      "INFO:tensorflow:loss = 0.140926, step = 5901 (47.881 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08568\n",
      "INFO:tensorflow:loss = 0.32377, step = 6001 (47.946 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08171\n",
      "INFO:tensorflow:loss = 0.132264, step = 6101 (48.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08129\n",
      "INFO:tensorflow:loss = 0.130649, step = 6201 (48.047 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6227 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.07686\n",
      "INFO:tensorflow:loss = 0.0840394, step = 6301 (48.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08567\n",
      "INFO:tensorflow:loss = 0.206137, step = 6401 (47.946 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08667\n",
      "INFO:tensorflow:loss = 0.148691, step = 6501 (47.923 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07887\n",
      "INFO:tensorflow:loss = 0.138925, step = 6601 (48.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08024\n",
      "INFO:tensorflow:loss = 0.311027, step = 6701 (48.071 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07819\n",
      "INFO:tensorflow:loss = 0.138882, step = 6801 (48.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09453\n",
      "INFO:tensorflow:loss = 0.19892, step = 6901 (47.743 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08655\n",
      "INFO:tensorflow:loss = 0.150631, step = 7001 (47.926 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08451\n",
      "INFO:tensorflow:loss = 0.0701188, step = 7101 (47.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09101\n",
      "INFO:tensorflow:loss = 0.0980929, step = 7201 (47.824 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08606\n",
      "INFO:tensorflow:loss = 0.0361862, step = 7301 (47.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08547\n",
      "INFO:tensorflow:loss = 0.147882, step = 7401 (47.951 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7479 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.08626\n",
      "INFO:tensorflow:loss = 0.157351, step = 7501 (47.933 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08683\n",
      "INFO:tensorflow:loss = 0.193177, step = 7601 (47.920 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09029\n",
      "INFO:tensorflow:loss = 0.136348, step = 7701 (47.840 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.0409718, step = 7801 (47.852 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08231\n",
      "INFO:tensorflow:loss = 0.135771, step = 7901 (48.024 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09175\n",
      "INFO:tensorflow:loss = 0.147288, step = 8001 (47.807 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08284\n",
      "INFO:tensorflow:loss = 0.0937629, step = 8101 (48.011 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08854\n",
      "INFO:tensorflow:loss = 0.0887747, step = 8201 (47.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.081\n",
      "INFO:tensorflow:loss = 0.10269, step = 8301 (48.054 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08906\n",
      "INFO:tensorflow:loss = 0.21492, step = 8401 (47.869 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08899\n",
      "INFO:tensorflow:loss = 0.120486, step = 8501 (47.870 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09029\n",
      "INFO:tensorflow:loss = 0.0995622, step = 8601 (47.840 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09249\n",
      "INFO:tensorflow:loss = 0.104155, step = 8701 (47.790 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8732 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.08666\n",
      "INFO:tensorflow:loss = 0.121961, step = 8801 (47.923 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08963\n",
      "INFO:tensorflow:loss = 0.0722865, step = 8901 (47.856 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09032\n",
      "INFO:tensorflow:loss = 0.225911, step = 9001 (47.840 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08846\n",
      "INFO:tensorflow:loss = 0.211502, step = 9101 (47.882 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.085\n",
      "INFO:tensorflow:loss = 0.11898, step = 9201 (47.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08736\n",
      "INFO:tensorflow:loss = 0.132935, step = 9301 (47.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08881\n",
      "INFO:tensorflow:loss = 0.0415243, step = 9401 (47.874 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09053\n",
      "INFO:tensorflow:loss = 0.0229505, step = 9501 (47.835 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08006\n",
      "INFO:tensorflow:loss = 0.145532, step = 9601 (48.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08604\n",
      "INFO:tensorflow:loss = 0.038726, step = 9701 (47.938 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08433\n",
      "INFO:tensorflow:loss = 0.142486, step = 9801 (47.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08157\n",
      "INFO:tensorflow:loss = 0.136851, step = 9901 (48.041 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9984 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.08289\n",
      "INFO:tensorflow:loss = 0.200421, step = 10001 (48.010 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08273\n",
      "INFO:tensorflow:loss = 0.0580604, step = 10101 (48.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08587\n",
      "INFO:tensorflow:loss = 0.0666358, step = 10201 (47.942 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08697\n",
      "INFO:tensorflow:loss = 0.140423, step = 10301 (47.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08435\n",
      "INFO:tensorflow:loss = 0.0663914, step = 10401 (47.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09287\n",
      "INFO:tensorflow:loss = 0.0528223, step = 10501 (47.781 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08408\n",
      "INFO:tensorflow:loss = 0.0558123, step = 10601 (47.983 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09161\n",
      "INFO:tensorflow:loss = 0.0689758, step = 10701 (47.810 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08591\n",
      "INFO:tensorflow:loss = 0.0454969, step = 10801 (47.941 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08599\n",
      "INFO:tensorflow:loss = 0.0383261, step = 10901 (47.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09021\n",
      "INFO:tensorflow:loss = 0.230477, step = 11001 (47.842 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09261\n",
      "INFO:tensorflow:loss = 0.103894, step = 11101 (47.788 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08939\n",
      "INFO:tensorflow:loss = 0.119454, step = 11201 (47.860 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11237 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.08228\n",
      "INFO:tensorflow:loss = 0.0918686, step = 11301 (48.024 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09009\n",
      "INFO:tensorflow:loss = 0.0552477, step = 11401 (47.845 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0948\n",
      "INFO:tensorflow:loss = 0.0923464, step = 11501 (47.737 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08281\n",
      "INFO:tensorflow:loss = 0.0924783, step = 11601 (48.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.086\n",
      "INFO:tensorflow:loss = 0.233372, step = 11701 (47.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08607\n",
      "INFO:tensorflow:loss = 0.0608598, step = 11801 (47.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09261\n",
      "INFO:tensorflow:loss = 0.0909989, step = 11901 (47.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0869\n",
      "INFO:tensorflow:loss = 0.0499153, step = 12001 (47.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08067\n",
      "INFO:tensorflow:loss = 0.136874, step = 12101 (48.061 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08706\n",
      "INFO:tensorflow:loss = 0.0386591, step = 12201 (47.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09757\n",
      "INFO:tensorflow:loss = 0.0497216, step = 12301 (47.674 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08339\n",
      "INFO:tensorflow:loss = 0.0557344, step = 12401 (47.999 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12490 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.08759\n",
      "INFO:tensorflow:loss = 0.0998722, step = 12501 (47.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08278\n",
      "INFO:tensorflow:loss = 0.041735, step = 12601 (48.013 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08674\n",
      "INFO:tensorflow:loss = 0.110588, step = 12701 (47.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08795\n",
      "INFO:tensorflow:loss = 0.109584, step = 12801 (47.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08384\n",
      "INFO:tensorflow:loss = 0.0886375, step = 12901 (47.989 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08714\n",
      "INFO:tensorflow:loss = 0.0901821, step = 13001 (47.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08234\n",
      "INFO:tensorflow:loss = 0.135677, step = 13101 (48.023 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08714\n",
      "INFO:tensorflow:loss = 0.0933889, step = 13201 (47.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08716\n",
      "INFO:tensorflow:loss = 0.0692896, step = 13301 (47.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0885\n",
      "INFO:tensorflow:loss = 0.0706286, step = 13401 (47.881 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08748\n",
      "INFO:tensorflow:loss = 0.0491506, step = 13501 (47.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08363\n",
      "INFO:tensorflow:loss = 0.111751, step = 13601 (47.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09092\n",
      "INFO:tensorflow:loss = 0.0647763, step = 13701 (47.826 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13742 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.0903\n",
      "INFO:tensorflow:loss = 0.0390277, step = 13801 (47.840 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09115\n",
      "INFO:tensorflow:loss = 0.0696077, step = 13901 (47.820 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08957\n",
      "INFO:tensorflow:loss = 0.0155032, step = 14001 (47.857 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08415\n",
      "INFO:tensorflow:loss = 0.0665174, step = 14101 (47.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09177\n",
      "INFO:tensorflow:loss = 0.0503429, step = 14201 (47.806 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08989\n",
      "INFO:tensorflow:loss = 0.12817, step = 14301 (47.849 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09311\n",
      "INFO:tensorflow:loss = 0.0458144, step = 14401 (47.776 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08606\n",
      "INFO:tensorflow:loss = 0.187121, step = 14501 (47.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08447\n",
      "INFO:tensorflow:loss = 0.0551415, step = 14601 (47.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08953\n",
      "INFO:tensorflow:loss = 0.0132625, step = 14701 (47.858 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09024\n",
      "INFO:tensorflow:loss = 0.0188783, step = 14801 (47.841 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09226\n",
      "INFO:tensorflow:loss = 0.0783006, step = 14901 (47.795 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 14996 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.08504\n",
      "INFO:tensorflow:loss = 0.0666373, step = 15001 (47.961 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08571\n",
      "INFO:tensorflow:loss = 0.0593506, step = 15101 (47.945 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0929\n",
      "INFO:tensorflow:loss = 0.134438, step = 15201 (47.781 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08555\n",
      "INFO:tensorflow:loss = 0.036569, step = 15301 (47.949 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08805\n",
      "INFO:tensorflow:loss = 0.0621549, step = 15401 (47.891 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08911\n",
      "INFO:tensorflow:loss = 0.0958577, step = 15501 (47.867 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.109856, step = 15601 (47.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0874\n",
      "INFO:tensorflow:loss = 0.067357, step = 15701 (47.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08787\n",
      "INFO:tensorflow:loss = 0.0104552, step = 15801 (47.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08541\n",
      "INFO:tensorflow:loss = 0.15509, step = 15901 (47.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08577\n",
      "INFO:tensorflow:loss = 0.0550382, step = 16001 (47.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09097\n",
      "INFO:tensorflow:loss = 0.041399, step = 16101 (47.825 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0935\n",
      "INFO:tensorflow:loss = 0.0508644, step = 16201 (47.767 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 16249 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.08956\n",
      "INFO:tensorflow:loss = 0.106307, step = 16301 (47.857 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08646\n",
      "INFO:tensorflow:loss = 0.0284478, step = 16401 (47.928 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09258\n",
      "INFO:tensorflow:loss = 0.0416905, step = 16501 (47.788 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08711\n",
      "INFO:tensorflow:loss = 0.068975, step = 16601 (47.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08594\n",
      "INFO:tensorflow:loss = 0.0932166, step = 16701 (47.940 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0893\n",
      "INFO:tensorflow:loss = 0.0564233, step = 16801 (47.863 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08971\n",
      "INFO:tensorflow:loss = 0.0506733, step = 16901 (47.853 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09096\n",
      "INFO:tensorflow:loss = 0.150194, step = 17001 (47.825 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08129\n",
      "INFO:tensorflow:loss = 0.184082, step = 17101 (48.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08619\n",
      "INFO:tensorflow:loss = 0.151123, step = 17201 (47.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08982\n",
      "INFO:tensorflow:loss = 0.0805615, step = 17301 (47.851 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08613\n",
      "INFO:tensorflow:loss = 0.0829891, step = 17401 (47.936 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08213\n",
      "INFO:tensorflow:loss = 0.04797, step = 17501 (48.027 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 17502 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.07584\n",
      "INFO:tensorflow:loss = 0.0323059, step = 17601 (48.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08481\n",
      "INFO:tensorflow:loss = 0.0682759, step = 17701 (47.966 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08642\n",
      "INFO:tensorflow:loss = 0.0462115, step = 17801 (47.929 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08972\n",
      "INFO:tensorflow:loss = 0.0247368, step = 17901 (47.853 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09139\n",
      "INFO:tensorflow:loss = 0.044706, step = 18001 (47.815 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08372\n",
      "INFO:tensorflow:loss = 0.0201689, step = 18101 (47.991 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09496\n",
      "INFO:tensorflow:loss = 0.0434243, step = 18201 (47.734 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09015\n",
      "INFO:tensorflow:loss = 0.0476353, step = 18301 (47.844 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0841\n",
      "INFO:tensorflow:loss = 0.147079, step = 18401 (47.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08288\n",
      "INFO:tensorflow:loss = 0.116237, step = 18501 (48.011 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08137\n",
      "INFO:tensorflow:loss = 0.0125104, step = 18601 (48.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0838\n",
      "INFO:tensorflow:loss = 0.0126078, step = 18701 (47.989 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 18754 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.08835\n",
      "INFO:tensorflow:loss = 0.0465158, step = 18801 (47.885 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08769\n",
      "INFO:tensorflow:loss = 0.0477588, step = 18901 (47.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09089\n",
      "INFO:tensorflow:loss = 0.0296053, step = 19001 (47.826 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08974\n",
      "INFO:tensorflow:loss = 0.0185798, step = 19101 (47.853 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09375\n",
      "INFO:tensorflow:loss = 0.0902297, step = 19201 (47.761 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09052\n",
      "INFO:tensorflow:loss = 0.265625, step = 19301 (47.835 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09139\n",
      "INFO:tensorflow:loss = 0.019657, step = 19401 (47.815 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0833\n",
      "INFO:tensorflow:loss = 0.0598512, step = 19501 (48.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08618\n",
      "INFO:tensorflow:loss = 0.0712471, step = 19601 (47.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09197\n",
      "INFO:tensorflow:loss = 0.0219334, step = 19701 (47.802 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09225\n",
      "INFO:tensorflow:loss = 0.0591076, step = 19801 (47.796 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08988\n",
      "INFO:tensorflow:loss = 0.0267873, step = 19901 (47.849 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 20000 into ./model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0623579.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f9a5e3a9278>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': imgs_train}, y=labels_train_class,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-01-21-02:08:40\n",
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-20000\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-21-02:08:44\n",
      "INFO:tensorflow:Saving dict for global step 20000: accuracy = 0.539705, global_step = 20000, loss = 5.04987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.53970468, 'global_step': 20000, 'loss': 5.0498714}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': imgs_test}, y=labels_test_class,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-20000\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the images class\n",
    "preds = list(model.predict(input_fn))\n",
    "\n",
    "# Display\n",
    "#for i in range(n_images):\n",
    "#    plt.imshow(np.reshape(test_images[i], [img_size, img_size]), cmap='gray')\n",
    "#    plt.show()\n",
    "#    print(\"Model prediction:\", class_labels[preds[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cls_pred, cls_true):\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "    \n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_true,\n",
    "                          y_pred=cls_pred)\n",
    "\n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix as an image.\n",
    "    plt.matshow(cm)\n",
    "\n",
    "    # Make various adjustments to the plot.\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, range(num_classes))\n",
    "    plt.yticks(tick_marks, range(num_classes))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[184   5  69  33 105  13  82]\n",
      " [  7  23   7   2   9   2   5]\n",
      " [ 49   3 218  29 119  40  70]\n",
      " [ 25   0  39 636  80  27  72]\n",
      " [ 62   1  78  62 276  13 102]\n",
      " [ 14   1  43  24  28 281  25]\n",
      " [ 52   2  54  59 124  16 319]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAD3CAYAAADormr9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE4aZZfv/V4ABw/QxnWAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9a0cf9a908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cls_pred=preds, cls_true=labels_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAD3CAYAAADormr9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAF1FJREFUeJzt3X20XXV95/H35yZAyAMEDDI0SQtC\nCqWs4UFWxDLDKmApWsdgl06xLSJlJmtaanFwpqWdWct21swabWdpdeqwpIkWKz41yCJLGRARi3YJ\nkgTkKTBGinIBDQjECELIvZ/5Y+8Lx3Bz7z4353fO2fd+XmvtlbP32Wd/97k3+eb3tH8/2SYioomR\nQd9ARLRHEkZENJaEERGNJWFERGNJGBHRWBJGRDSWhBERjSVhRERjSRgR0VgSRkQ0Nn/QNxAxl/36\nmYv8o6fGGp27+e4XbrR9buFbmlISRsQAPfnUGLffuKLRufsd8d1lhW9nWkkYEQNlxjw+6JtoLAkj\nYoAMjNOeJ8aTMCIGyJgX3awNYxi0tpdE0rmSHpS0TdLlhWJ8XNJ2SfeWuH4dY6WkWyRtlXSfpEsL\nxFgg6VuSvl3H+Itex9gj3jxJd0r6YqHrPyzpHkl3SdpUIkYdZ6mkDZIeqH8/ry8RZxw32oZBKxOG\npHnAR4E3AscD75B0fIFQfweUbpXeDbzX9i8BpwGXFPguLwBn2T4ROAk4V9JpPY7R6VJga8HrA5xp\n+yTbpxaM8WHgBtvHASdS4DsZGMONtmHQyoQBrAa22X7I9i7gs8CaXgexfSvwVK+vu0eMx21vqV/v\npPpLubzHMWz7J/XufvVW5G+gpBXAbwDrSly/XyQdBJwBrAewvcv2MyVipYRR3nLgkY79UXr8j2wQ\nJB0JnAzcXuDa8yTdBWwHbrLd8xi1vwb+GCjZ9G/gy5I2S1pbKMZrgCeAT9TVq3WSFvU6iIExu9E2\nDNqaMDTJseH4ic6QpMXANcB7bP+419e3PWb7JGAFsFrSCb2OIenNwHbbm3t97T2cbvsUqirpJZLO\nKBBjPnAKcIXtk4FngSJtZeMNt2HQ1oQxCqzs2F8BPDage9lnkvajShZX2/5CyVh1sfprlGmbOR14\ni6SHqaqJZ0n6VK+D2H6s/nM7cC1VFbXXRoHRjpLYBqoE0lNu2H6RNox9cwewStJRkvYHzgc2Dvie\nZkSSqOrJW21/sFCMwyQtrV8fCLwBeKDXcWz/qe0Vto+k+p181fbv9jKGpEWSlky8Bs4Bet6LZfsH\nwCOSjq0PnQ3c3/s48GLDbRi0chyG7d2S/hC4EZgHfNz2fb2OI+kzwK8CyySNAu+zvb7HYU4HLgDu\nqdsYAP7M9vU9jHEEcFXduzQCfN52kS7PPjgcuLbKs8wHPm37hkKx3g1cXf+n9BBwUe9DiLFJa9jD\nSVmXJGJwTviX+/uaLzV7ROS4n398c+Fu5Gm1soQRMZu0qYSRhBExQNXArSSMiGho3EkYEdFAShgR\n0ZgRL3reoG+jsbaOwwCg4LDgvsfJdxnOOKVjTJQwmmzDoNUJA+jLX8w+xcl3Gc44hWOIMY802oZB\nqiQRA1TNuDUcyaCJoUoY++2/yAsWHtL4/AMOXMqSpSu6HnmmHc91df4CFnKQDi06wm2mMeoRj83j\naBEHj7yqqzjd3tQCFnJQlzGqQN19ZCY/s378vH7qZ9nl5xsHGpbqRhNDlTAWLDyEk//1HxWPc8D1\nxSZpelmfRtCOLFhQPIbH+vOspHe/WDzGyAEHFI9x2/PNR/Xb6ml1Q9J/BP4dVZ6/h2o4+xFUDwMe\nCmwBLrC9S9IBwCeB1wI/An7L9sNTXb89ZaGIWWocNdqmI2k58EfAqbZPoHrO6nzgA8CHbK8CngYu\nrj9yMfC07WOAD9XnTSkJI2KAjNjl+Y22huYDB0qaDywEHgfOono8H+Aq4Lz69Zp6n/r9szVNnS0J\nI2KAJho9m2zTXst+FPhfwPepEsUOYDPwjO3d9Wmds9O9NHNd/f4O4FVTxUjCiBiwMavRRjXNwqaO\n7We6fCUdQlVqOAr4OWAR1axke5poYOt65rqhavSMmGuMGGv+//aT0zze/gbgn20/ASDpC8CvAEsl\nza9LEZ2z003MXDdaV2EOZppJr1PCiBiwcY802hr4PnCapIV1W8TELGG3AG+rz7kQuK5+vbHep37/\nq55mgpyUMCIGqBoa3pv/t23fLmkDVdfpbuBO4ErgS8BnJf33+tjErHHrgb+XtI2qZHH+dDGKJgxJ\n51ItBjMPWGf7/SXjRbRNrx8+s/0+4H17HH6ISSZKtv088PZurl8sYXSsTvZrVHWlOyRttN3ziVQj\n2spmaJ4TaaLknfZldbKIdms2aKvJwK1+KFklmWx1stcVjBfROtXKZ+0pYZRMGI36eOu+5LVQPUwW\nMdf0qtGzH0omjEark9m+kqold0ZPnka0mVHm9Ky9tDoZ8ChVl81vF4wX0UopYdC/1cki2qxtc3oW\nHYdRL/fXyyX/ImYVQ9NRnEMhIz0jBiwzbkVEI7ZSwoiI5jIOIyIaqSbQSZUkIhrp7STApSVhRAyQ\nId2qEdFMRnruA+14jgO+dEf5QCN9yOgeKx8DGH/++b7EmS368fOaZtKqV8jKZxHRSDUfRkoYEdFQ\nqiQR0UjVhpEqSUQ0lKHhEdGIEbvH060aEQ1lpGdENJJekojoSpsaPYvdqaSPS9ou6d5SMSLabmKk\nZ5NtGJRMbX8HnFvw+hGzQtYlAWzfKunIUtePmA2qKfqGIxk0kTaMiEFyulW70rmQ0QIWDvhuIvor\nE+h0qXMho4N0aBYyijknVZKIaKRtbRglu1U/A3wTOFbSqKSLS8WKaLM2dauW7CV5R6lrR8wWmXEr\nIpoz7G7RSM8kjIgBalsbRhJGxIAlYUREI2nDiIiuOAkjIprKSM+IaMROG8a+UR9+eOPlFxkaWbKk\neAyA8Z07ywfpx++kX7pcZKg8MTbeu25VSUuBdcAJVJ0wvwc8CHwOOBJ4GPi3tp+WJODDwJuA54B3\n2d4y1fXb0wEcMUvZarQ19GHgBtvHAScCW4HLgZttrwJurvcB3gisqre1wBXTXTwJI2KAJsZh9GJo\nuKSDgDOA9QC2d9l+BlgDXFWfdhVwXv16DfBJV24Dlko6YqoYSRgRg+SqltRka+A1wBPAJyTdKWmd\npEXA4bYfB6j/fHV9/nLgkY7Pj9bH9ioJI2LAupiib5mkTR3b2j0uNR84BbjC9snAs7xc/ZjMZMWW\nKVPT8DV6RswhpqtxGE/aPnWK90eBUdu31/sbqBLGDyUdYfvxusqxveP8lR2fXwE8NtUNpIQRMVC9\nmzXc9g+ARyQdWx86G7gf2AhcWB+7ELiufr0ReKcqpwE7Jqoue5MSRsSAjY/3tNv63cDVkvYHHgIu\noioYfL6ek+b7wNvrc6+n6lLdRtWtetF0F0/CiBigqkGzdwnD9l3AZNWWsyc518Al3Vw/CSNiwNo0\n0rPkFH0rJd0iaauk+yRdWipWRJv1sFu1uJIljN3Ae21vkbQE2CzpJtv3F4wZ0Tp5WpWXBohMDBbZ\nKWkr1aCQJIyImulq2PfA9aUNo14y8WTg9qnPjJh7hqS20UjxhCFpMXAN8B7bP57k/ax8FnOXwb3t\nVi2qaMKQtB9Vsrja9hcmOycrn8VclyoJUD9rvx7YavuDpeJEtN2w9IA0UXJo+OnABcBZku6qtzcV\njBfROhPPkvRwPoyiSvaSfIPJn4aLiAkGhiQZNJGRnhED1qYqSRJGxKAlYUREM0q3akQ01OOnVUtL\nwogYtFRJIqK5lDAioqmUMGZGIyOMLCz/PMn4s8+Wj9GPFcmAt97/RPEY152ycvqTesBj48VjjCxe\nVDyGdszr7gNJGBHRSB4+i4iuzMYShqQDbL9Q8mYi5qQWdatO+/CZpNWS7gG+U++fKOl/F7+ziDlC\nbrYNgyZPq34EeDPwIwDb3wbOLHlTEXOGu9iGQJMqyYjt71XTW7xkrND9RMwxalWVpEnCeETSasCS\n5lGtrPT/yt5WxBwyJKWHJpokjN+nqpb8PPBD4Cv1sYjohfLDT3pm2oRheztwfrcXlrQAuBU4oI6z\nwfb7ur7DiNlstk2gI+lvmaTQZHvtNB99ATjL9k/qyYC/Ien/2r5tZrcaMTsNSw9IE02qJF/peL0A\neCvwyHQfqhd6/Um9u1+9tehHE9EnLfpX0aRK8rnOfUl/D9zU5OJ1I+lm4Bjgo7azkFFEi81k1vCj\ngF9ocqLtMdsnASuA1ZJO2PMcSWslbZK0aZefn8HtRLRbmwZuNWnDeJqXC00jwFPA5d0Esf2MpK8B\n5wL37vHeSwsZHTxv2ZD8WCL6aLY0etaLEZ0IPFofGq/bJqYl6TDgxTpZHAi8AfjAvtxsxKxjZk+3\nqm1Lutb2a2dw7SOAq+p2jBHg87a/OJObjJjNhqW60USTXpJvSTrF9pZuLmz7bqoV2yNiKrMhYUia\nb3s38K+Afy/pu8CzVBMQ2vYpfbrHiNltNiQM4FvAKcB5fbqXiDlnmHpAmpgqYQjA9nf7dC8Rc9Ms\n6SU5TNJle3vT9gcL3E/E3DNLShjzgMW0adGEiBbSLOlWfdz2f+vbnUTMRQXaMOqhDJuAR22/WdJR\nwGeBQ4EtwAW2d0k6APgk8FqqGfV+y/bDU117qqHhKVlE9EPvp+i7FNjasf8B4EO2VwFPAxfXxy8G\nnrZ9DPAhGgysnKqEcXZXt9gLAs3rchGYGcUpnws1f7/iMQA2nnZ08Rg71hxfPAbAkn+4o3iM8aOX\nF4/h+7r83fewhCFpBfAbwP8ALqtHa58F/HZ9ylXAnwNXAGvq1wAbgL+RpKlGc++1hGH7qX29+YiY\nXo8fPvtr4I95ecD5q4Bn6jFVAKPARNZcTj1VRf3+jvr8vZrJ06oRMRjLJp7srrefmcRK0puB7bY3\ndx6e5Dpu8N6ksvJZxKA1Lz08afvUKd4/HXiLpDdRTXZ1EFWJY2nHyO0VwGP1+aPASmBU0nzgYKqn\n0fcqJYyIQXLVrdpkm/ZS9p/aXmH7SKp5eL9q+3eAW4C31addCFxXv95Y71O//9XpnkZPwogYtPIL\nGf0JVQPoNqo2ivX18fXAq+rjl9FgnptUSSIGSJR5lsT214Cv1a8fAlZPcs7zwNu7uW4SRsSgzZKh\n4RFR2ix6WjUi+qFFCaN4o6ekeZLulJTp+SIm0atekn7oRy/JnuPaI6JT+V6SnimaMDrGta8rGSei\ntZomiyFJGKXbMCbGtS8pHCeitdrU6FmshLGXce2TnffyymfjWfks5qAWlTBKVkkmxrU/TDV5x1mS\nPrXnSbavtH2q7VP3H1lQ8HYihlOblkosljD2Mq79d0vFi2itFpUwMg4jYoCGqfTQRF8SRue49ojY\nQxJGRDSVEkZENJeEERGNJWFERCNp9IyIriRhRERTw/IkahNJGBEDlirJDHncjL/wQh8Clf8NjSxe\nVDwGgPvw8zromk3FYwA8etnrisdYue6+4jH0wovNTx6iUZxNDFXCiJiTkjAioolSs4aXkoQRMWhJ\nGBHRlPrQptYrSRgRg+R0q0ZEN9pTwEjCiBi0NHpGRHNJGJV6Ps+dwBiw2/apJeNFtE4ePnuFM20/\n2Yc4Ee2UhBERTbRt4FbppRINfFnSZklrC8eKaCWNu9E2DEqXME63/ZikVwM3SXrA9q2dJ9SJZC3A\nAhYWvp2IIdOyh8+KljBsP1b/uR24Flg9yTkvLWS0n7KQUcw9Wb0dkLRI0pKJ18A5wL2l4kW0VhYy\nAuBw4FpJE3E+bfuGgvEiWqlNjZ7FEobth4ATS10/YlYwfZnQqVfSrRoxYMPSPtFEEkbEALVtHEYS\nRsQg2a2qkpQeuBUR05hYwX26bdrrSCsl3SJpq6T7JF1aHz9U0k2SvlP/eUh9XJI+ImmbpLslnTJd\njCSMiEHrXbfqbuC9tn8JOA24RNLxwOXAzbZXATfX+wBvBFbV21rgiukCJGFEDFivShi2H7e9pX69\nE9gKLAfWAFfVp10FnFe/XgN80pXbgKWSjpgqRhJGxCAZGHezrQuSjgROBm4HDrf9OFRJBXh1fdpy\n4JGOj43Wx/ZqqBo9NW+EkSWLi8cZe3F38RjjO3cWjwEw77BlxWPsOvpfFI8BsOJj9xSPcf2DXy8e\nY/Wvd/e776JbdZmkzlWlrrR95SuuJy0GrgHeY/vH9eDJSUNPcmzKzDRUCSNiTmreS/LkdJNQSdqP\nKllcbfsL9eEfSjrC9uN1lWN7fXwUWNnx8RXAY1NdP1WSiAHrYS+JgPXAVtsf7HhrI3Bh/fpC4LqO\n4++se0tOA3ZMVF32JiWMiEHq7YNlpwMXAPdIuqs+9mfA+4HPS7oY+D7w9vq964E3AduA54CLpguQ\nhBExQNVIz95kDNvfYPJ2CYCzJznfwCXdxEjCiBi0PEsSEU1lqcSIaMbdj7EYpCSMiAHL06oR0VyL\nqiRFx2FIWippg6QH6ifoXl8yXkTruF2TAJcuYXwYuMH22yTtD1lHIOIVWlTCKJYwJB0EnAG8C8D2\nLmBXqXgRrdWefFG0SvIa4AngE5LulLSuXm4gIjrIbrQNg5IJYz5wCnCF7ZOBZ3l54o6XSForaZOk\nTbvGny94OxFDyMCYm21DoGTCGAVGbd9e72+gSiA/o3Pls/1HsvJZzC2iWeli1pcwbP8AeETSsfWh\ns4H7S8WLaK2JiYCn24ZA6V6SdwNX1z0kD9HgabiIOWdIkkETRROG7buAKSf8iJjTTB4+i4jmhqV9\nookkjIhBS8KIiEZsGG9PnSQJI2LQ2pMvkjAiBi1tGBHRXBJGRDQysfJZSwxVwvjx7iefvPGJj32v\ni48sA54sdT/7FGesDzFgmmVnehSnHzFmpus486ZcObQ3MYBfaH7q8IzibGKoEobtw7o5X9Km6VaC\n6oV+xMl3Gc44ffkuSRgR0YiBsfZ0kyRhRAyUwUkY/fKKlatbHCffZTjjlI/RoipJqxdjnmyp+7bG\n6YwhaUzSXZLulfQPkmY8F6qkX5X0xfr1W4BDpzh3qaQ/mEGMP5f0nyb2Z+vvpUwAql6SJtsQaHXC\nmMV+avsk2ydQzYP6HzrfrFfb7vp3Z3uj7fdPccpSoOuEEfuoRfNhJGEMv68Dx0g6sl6q4f8AW4CV\nks6R9E1JW+qSyGIASefWSzt8A/jNiQtJepekv6lfHy7pWknfrrdfoVrl++i6dPNX9Xn/WdIdku6W\n9Bcd1/ovkh6U9BXgWGLmWpQw2t6GMatJmg+8EbihPnQscJHtP5C0DPivwBtsPyvpT4DLJP0l8LfA\nWcA24HN7ufxHgH+0/VZJ84DFVHOunmD7pDr+OcAqYDXVquAbJZ1BNT/r+cDJVH+HtgCbe/vt5wgb\nxroftDMoSRjD6UBJd9Wvvw6sB34O+J7t2+rjpwHHA/8kCWB/4JvAccA/2/4OgKRPAWsniXEW8E4A\n22PADkmH7HHOOfV2Z72/mCqBLAGutf1cHWPjPn3buW5ISg9NJGEMp59O/C8/oU4Kz3YeAm6y/Y49\nzjuJ3q10IeB/2v7YHjHe08MY0aKEkTaM9roNOF3SMQCSFkr6ReAB4ChJR9fnvWMvn78Z+P36s/Pq\nhad2UpUeJtwI/F5H28hySa8GbgXeKulASUuAf9Pj7zaHNOwhSS9J7AvbT1CtKvcZSXdTJZDjbD9P\nVQX5Ut3oubdncy4FzpR0D1X7wy/b/hFVFedeSX9l+8vAp4Fv1udtAJbY3kLVNnIXcA1VtSlmwmCP\nN9qGgdyi4lDEbHPw/MP8+oPOa3TujU+v29yPZ3SmkjaMiEFr0X/aSRgRg5Ru1YjohjMJcEQ0Mzyj\nOJtIwogYpJZN0Zdu1YhB83izrYH6OaIHJW2TdHmvbzUljIgBMuAelTDqZ4I+CvwaMArcIWmj7ft7\nEoCUMCIGy+5lCWM1sM32Q7Z3AZ8F1vTydlPCiBgw965bdTnwSMf+KPC6Xl0ckjAiBmonT9/4FW9Y\n1vD0BZI2dexfuceMYJrkMz1tUU3CiBgg2+f28HKjwMqO/RXMZFWZKaQNI2L2uANYJekoSftTTXLU\n07lKUsKImCVs75b0h1TTEswDPm77vl7GyNOqEdFYqiQR0VgSRkQ0loQREY0lYUREY0kYEdFYEkZE\nNJaEERGNJWFERGP/HyF2AX9IoRH5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea71954860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = [[213,   5,   2,  22,  16,   6,  24],\n",
    " [  0,   8,   0,   0,   0,   0,   1],\n",
    " [  4,   0,  34,   2,   5,   6,   3],\n",
    " [ 30,   2,   1, 769,  24,  16,  43],\n",
    " [ 37,   7,  17,  38, 250,   8, 139],\n",
    " [ 20,   2,  44,  25,  13, 353,  41],\n",
    " [ 53,  10,  13,  66, 164,  34, 912]]\n",
    "\n",
    "# Plot the confusion matrix as an image.\n",
    "plt.matshow(cm)\n",
    "\n",
    "# Make various adjustments to the plot.\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(7)\n",
    "plt.xticks(tick_marks, range(7))\n",
    "plt.yticks(tick_marks, range(7))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "# Ensure the plot is shown correctly with multiple plots\n",
    "# in a single Notebook cell.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
