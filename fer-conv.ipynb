{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition\n",
    "In this approach, we will setup a convolutional neural network to aim for an accuracy > 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from random import randint\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "writer = tf.summary.FileWriter('logs/graph', session.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# images are 48x48 pixels\n",
    "img_size = 48\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "num_classes = 7\n",
    "class_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# probability to drop a unit (prevent overfitting)\n",
    "dropout = 0.25\n",
    "\n",
    "# training parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 6000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Vectors (0-255)\n",
    "imgs_train_byte = []\n",
    "imgs_test_byte = []\n",
    "\n",
    "# Labels (0-6)\n",
    "labels_train_class = []\n",
    "labels_test_class = []\n",
    "\n",
    "\n",
    "with open('fer2013.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    # skip CSV header\n",
    "    next(readCSV)\n",
    "    for row in readCSV:\n",
    "        if row[2] == 'Training':\n",
    "            # cast pixels to int\n",
    "            pixels_train = [float(x) for x in row[1].split(' ')]\n",
    "            imgs_train_byte.append(np.array(pixels_train))\n",
    "            labels_train_class.append(int(row[0]))\n",
    "        elif row[2] == 'PrivateTest':\n",
    "            pixels_test = [float(x) for x in row[1].split(' ')]\n",
    "            imgs_test_byte.append(np.array(pixels_test))\n",
    "            labels_test_class.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cast to correct type and wrap into np arrays\n",
    "imgs_train_byte = np.array(imgs_train_byte, dtype=np.float32)\n",
    "imgs_test_byte = np.array(imgs_test_byte, dtype=np.float32)\n",
    "labels_train_class = np.array(labels_train_class, dtype=np.float32)\n",
    "labels_test_class = np.array(labels_test_class, dtype=np.float32)\n",
    "# normalize the pixel intensitys\n",
    "imgs_train = np.divide(imgs_train_byte, 255)\n",
    "imgs_test = np.divide(imgs_test_byte, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    \n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "\n",
    "        # The data input is a 1-D vector of 2304 features (48*48 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, img_size, img_size, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "        \n",
    "        # Convolution Layer with 128 filters and a kernel size of 3\n",
    "        conv3 = tf.layers.conv2d(conv2, 128, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv3 = tf.layers.max_pooling2d(conv3, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv3)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model using the Estimator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './model', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Build the Estimator\n",
    "model = tf.estimator.Estimator(model_fn=model_fn, model_dir='./model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2f23082363b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m243\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "f = open(\"test.txt\",\"w\") \n",
    "f.write(imgs_train[243])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.94394, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1.89395\n",
      "INFO:tensorflow:loss = 1.68552, step = 101 (52.801 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.75434\n",
      "INFO:tensorflow:loss = 1.62461, step = 201 (57.002 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.55893\n",
      "INFO:tensorflow:loss = 1.38268, step = 301 (64.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.53913\n",
      "INFO:tensorflow:loss = 1.46083, step = 401 (64.971 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.6434\n",
      "INFO:tensorflow:loss = 1.26303, step = 501 (60.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.06608\n",
      "INFO:tensorflow:loss = 1.32979, step = 601 (48.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0556\n",
      "INFO:tensorflow:loss = 1.2124, step = 701 (48.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0293\n",
      "INFO:tensorflow:loss = 1.24572, step = 801 (49.278 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.96754\n",
      "INFO:tensorflow:loss = 1.28423, step = 901 (50.825 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.02451\n",
      "INFO:tensorflow:loss = 1.33821, step = 1001 (49.395 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1101 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.8564\n",
      "INFO:tensorflow:loss = 1.05192, step = 1101 (53.868 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.75728\n",
      "INFO:tensorflow:loss = 1.17455, step = 1201 (56.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.76494\n",
      "INFO:tensorflow:loss = 1.12584, step = 1301 (56.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.79525\n",
      "INFO:tensorflow:loss = 1.08292, step = 1401 (55.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.85563\n",
      "INFO:tensorflow:loss = 1.18927, step = 1501 (53.890 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.79799\n",
      "INFO:tensorflow:loss = 1.0055, step = 1601 (55.618 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.86095\n",
      "INFO:tensorflow:loss = 1.02031, step = 1701 (53.736 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69971\n",
      "INFO:tensorflow:loss = 1.06833, step = 1801 (58.833 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11031\n",
      "INFO:tensorflow:loss = 0.875112, step = 1901 (47.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.85837\n",
      "INFO:tensorflow:loss = 1.14484, step = 2001 (53.811 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.98338\n",
      "INFO:tensorflow:loss = 1.08321, step = 2101 (50.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.05962\n",
      "INFO:tensorflow:loss = 1.06116, step = 2201 (48.553 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2219 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.09015\n",
      "INFO:tensorflow:loss = 1.03558, step = 2301 (47.844 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.10443\n",
      "INFO:tensorflow:loss = 0.893726, step = 2401 (47.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.05404\n",
      "INFO:tensorflow:loss = 1.01848, step = 2501 (48.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11661\n",
      "INFO:tensorflow:loss = 0.910456, step = 2601 (47.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12228\n",
      "INFO:tensorflow:loss = 0.998731, step = 2701 (47.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0887\n",
      "INFO:tensorflow:loss = 0.820128, step = 2801 (47.877 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94191\n",
      "INFO:tensorflow:loss = 0.875092, step = 2901 (51.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.95803\n",
      "INFO:tensorflow:loss = 0.812671, step = 3001 (51.072 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.7907\n",
      "INFO:tensorflow:loss = 0.87565, step = 3101 (55.844 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.90288\n",
      "INFO:tensorflow:loss = 0.892458, step = 3201 (52.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.90876\n",
      "INFO:tensorflow:loss = 0.776942, step = 3301 (52.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.81106\n",
      "INFO:tensorflow:loss = 0.716673, step = 3401 (55.216 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3408 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.88112\n",
      "INFO:tensorflow:loss = 0.876652, step = 3501 (53.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.9747\n",
      "INFO:tensorflow:loss = 0.835531, step = 3601 (50.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.97824\n",
      "INFO:tensorflow:loss = 0.956935, step = 3701 (50.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.00005\n",
      "INFO:tensorflow:loss = 0.794684, step = 3801 (49.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.73768\n",
      "INFO:tensorflow:loss = 0.92749, step = 3901 (57.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.93542\n",
      "INFO:tensorflow:loss = 0.73472, step = 4001 (51.669 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.88126\n",
      "INFO:tensorflow:loss = 0.780936, step = 4101 (53.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.10537\n",
      "INFO:tensorflow:loss = 0.546665, step = 4201 (47.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11404\n",
      "INFO:tensorflow:loss = 0.68243, step = 4301 (47.303 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09529\n",
      "INFO:tensorflow:loss = 0.797769, step = 4401 (47.726 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.92196\n",
      "INFO:tensorflow:loss = 0.524945, step = 4501 (52.030 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4584 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.91007\n",
      "INFO:tensorflow:loss = 0.580121, step = 4601 (52.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.95636\n",
      "INFO:tensorflow:loss = 0.727426, step = 4701 (51.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.99353\n",
      "INFO:tensorflow:loss = 0.504562, step = 4801 (50.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.97517\n",
      "INFO:tensorflow:loss = 0.619324, step = 4901 (50.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.7944\n",
      "INFO:tensorflow:loss = 0.610008, step = 5001 (55.729 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69919\n",
      "INFO:tensorflow:loss = 0.701604, step = 5101 (58.851 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.99317\n",
      "INFO:tensorflow:loss = 0.559546, step = 5201 (50.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.92771\n",
      "INFO:tensorflow:loss = 0.762541, step = 5301 (51.875 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.03751\n",
      "INFO:tensorflow:loss = 0.634106, step = 5401 (49.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94266\n",
      "INFO:tensorflow:loss = 0.633309, step = 5501 (51.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.94908\n",
      "INFO:tensorflow:loss = 0.648388, step = 5601 (51.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.97752\n",
      "INFO:tensorflow:loss = 0.418163, step = 5701 (50.568 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5744 into ./model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.98076\n",
      "INFO:tensorflow:loss = 0.566227, step = 5801 (50.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.87688\n",
      "INFO:tensorflow:loss = 0.425648, step = 5901 (53.280 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6000 into ./model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.477484.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f7749a05278>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': imgs_train}, y=labels_train_class,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-01-17-14:00:14\n",
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-6000\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-17-14:00:18\n",
      "INFO:tensorflow:Saving dict for global step 6000: accuracy = 0.514349, global_step = 6000, loss = 1.9501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5143494, 'global_step': 6000, 'loss': 1.9500998}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': imgs_test}, y=labels_test_class,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-6000\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the images class\n",
    "preds = list(model.predict(input_fn))\n",
    "\n",
    "# Display\n",
    "#for i in range(n_images):\n",
    "#    plt.imshow(np.reshape(test_images[i], [img_size, img_size]), cmap='gray')\n",
    "#    plt.show()\n",
    "#    print(\"Model prediction:\", class_labels[preds[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cls_pred, cls_true):\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "    \n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_true,\n",
    "                          y_pred=cls_pred)\n",
    "\n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix as an image.\n",
    "    plt.matshow(cm)\n",
    "\n",
    "    # Make various adjustments to the plot.\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, range(num_classes))\n",
    "    plt.yticks(tick_marks, range(num_classes))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[196  10  82  39  88  18  58]\n",
      " [ 13  22   5   3   5   4   3]\n",
      " [ 71   7 227  39  86  49  49]\n",
      " [ 43   7  65 618  57  30  59]\n",
      " [ 76   1 116  51 214  14 122]\n",
      " [ 15   3  66  20  15 276  21]\n",
      " [ 56   3  80  59 102  33 293]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAD0CAYAAABuOhhTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7703e172e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cls_pred=preds, cls_true=labels_test_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
