{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition\n",
    "In this approach, we will setup a convolutional neural network to aim for an accuracy > 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from random import randint\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "writer = tf.summary.FileWriter('logs/graph', session.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# images are 48x48 pixels\n",
    "img_size = 48\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "num_classes = 7\n",
    "class_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# probability to drop a unit\n",
    "dropout = 0.25\n",
    "\n",
    "# training parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 5000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Vectors (0-255)\n",
    "imgs_train_byte = []\n",
    "imgs_test_byte = []\n",
    "\n",
    "# Labels (0-6)\n",
    "labels_train_class = []\n",
    "labels_test_class = []\n",
    "\n",
    "\n",
    "with open('fer2013.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    # skip CSV header\n",
    "    next(readCSV)\n",
    "    for row in readCSV:\n",
    "        if row[2] == 'Training':\n",
    "            # cast pixels to int\n",
    "            pixels_train = [float(x) for x in row[1].split(' ')]\n",
    "            imgs_train_byte.append(np.array(pixels_train))\n",
    "            labels_train_class.append(int(row[0]))\n",
    "        elif row[2] == 'PrivateTest':\n",
    "            pixels_test = [float(x) for x in row[1].split(' ')]\n",
    "            imgs_test_byte.append(np.array(pixels_test))\n",
    "            labels_test_class.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cast to correct type and wrap into np arrays\n",
    "imgs_train_byte = np.array(imgs_train_byte, dtype=np.float32)\n",
    "imgs_test_byte = np.array(imgs_test_byte, dtype=np.float32)\n",
    "labels_train_class = np.array(labels_train_class, dtype=np.float32)\n",
    "labels_test_class = np.array(labels_test_class, dtype=np.float32)\n",
    "# normalize the pixel intensitys\n",
    "imgs_train = np.divide(imgs_train_byte, 255)\n",
    "imgs_test = np.divide(imgs_test_byte, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    \n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "\n",
    "        # The data input is a 1-D vector of 2304 features (48*48 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, img_size, img_size, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "        \n",
    "        # Convolution Layer with 128 filters and a kernel size of 3\n",
    "        conv3 = tf.layers.conv2d(conv2, 128, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv3 = tf.layers.max_pooling2d(conv3, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv3)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model using the Estimator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpcvnt3rp4\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpcvnt3rp4', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Build the Estimator\n",
    "model = tf.estimator.Estimator(model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpcvnt3rp4/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.93938, step = 1\n",
      "INFO:tensorflow:global_step/sec: 2.11937\n",
      "INFO:tensorflow:loss = 1.72064, step = 101 (47.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12062\n",
      "INFO:tensorflow:loss = 1.4903, step = 201 (47.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11872\n",
      "INFO:tensorflow:loss = 1.4232, step = 301 (47.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12522\n",
      "INFO:tensorflow:loss = 1.49745, step = 401 (47.054 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.99181\n",
      "INFO:tensorflow:loss = 1.51882, step = 501 (50.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09432\n",
      "INFO:tensorflow:loss = 1.29979, step = 601 (47.748 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.13954\n",
      "INFO:tensorflow:loss = 1.21132, step = 701 (46.739 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.87619\n",
      "INFO:tensorflow:loss = 1.08284, step = 801 (53.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.06341\n",
      "INFO:tensorflow:loss = 1.18852, step = 901 (48.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08198\n",
      "INFO:tensorflow:loss = 1.12387, step = 1001 (48.031 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.99363\n",
      "INFO:tensorflow:loss = 1.03808, step = 1101 (50.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.99771\n",
      "INFO:tensorflow:loss = 1.05858, step = 1201 (50.058 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1235 into /tmp/tmpcvnt3rp4/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.99857\n",
      "INFO:tensorflow:loss = 1.05468, step = 1301 (50.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11603\n",
      "INFO:tensorflow:loss = 1.20723, step = 1401 (47.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11998\n",
      "INFO:tensorflow:loss = 1.021, step = 1501 (47.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09859\n",
      "INFO:tensorflow:loss = 1.04153, step = 1601 (47.651 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11718\n",
      "INFO:tensorflow:loss = 1.02343, step = 1701 (47.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.90459\n",
      "INFO:tensorflow:loss = 0.785821, step = 1801 (52.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09722\n",
      "INFO:tensorflow:loss = 0.959396, step = 1901 (47.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08258\n",
      "INFO:tensorflow:loss = 0.972668, step = 2001 (48.017 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.95134\n",
      "INFO:tensorflow:loss = 0.978565, step = 2101 (51.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11538\n",
      "INFO:tensorflow:loss = 0.967603, step = 2201 (47.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11699\n",
      "INFO:tensorflow:loss = 0.851029, step = 2301 (47.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12118\n",
      "INFO:tensorflow:loss = 0.910133, step = 2401 (47.144 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2479 into /tmp/tmpcvnt3rp4/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.12078\n",
      "INFO:tensorflow:loss = 0.998486, step = 2501 (47.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.1141\n",
      "INFO:tensorflow:loss = 0.831614, step = 2601 (47.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11421\n",
      "INFO:tensorflow:loss = 1.02343, step = 2701 (47.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12654\n",
      "INFO:tensorflow:loss = 0.833065, step = 2801 (47.025 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12669\n",
      "INFO:tensorflow:loss = 0.84538, step = 2901 (47.021 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12283\n",
      "INFO:tensorflow:loss = 0.6968, step = 3001 (47.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12553\n",
      "INFO:tensorflow:loss = 0.742875, step = 3101 (47.047 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12278\n",
      "INFO:tensorflow:loss = 0.736168, step = 3201 (47.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12636\n",
      "INFO:tensorflow:loss = 0.733218, step = 3301 (47.029 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08446\n",
      "INFO:tensorflow:loss = 0.604159, step = 3401 (47.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.13461\n",
      "INFO:tensorflow:loss = 0.718332, step = 3501 (46.847 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.13202\n",
      "INFO:tensorflow:loss = 0.850801, step = 3601 (46.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.06144\n",
      "INFO:tensorflow:loss = 0.831665, step = 3701 (48.510 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3749 into /tmp/tmpcvnt3rp4/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.1142\n",
      "INFO:tensorflow:loss = 0.649347, step = 3801 (47.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12981\n",
      "INFO:tensorflow:loss = 0.636669, step = 3901 (46.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.1285\n",
      "INFO:tensorflow:loss = 0.762363, step = 4001 (46.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11709\n",
      "INFO:tensorflow:loss = 0.47644, step = 4101 (47.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.13579\n",
      "INFO:tensorflow:loss = 0.644437, step = 4201 (46.821 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.13227\n",
      "INFO:tensorflow:loss = 0.566401, step = 4301 (46.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08526\n",
      "INFO:tensorflow:loss = 0.558084, step = 4401 (47.956 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.88988\n",
      "INFO:tensorflow:loss = 0.696817, step = 4501 (52.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.06263\n",
      "INFO:tensorflow:loss = 0.700588, step = 4601 (48.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.13047\n",
      "INFO:tensorflow:loss = 0.609884, step = 4701 (46.938 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.1206\n",
      "INFO:tensorflow:loss = 0.57996, step = 4801 (47.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.1206\n",
      "INFO:tensorflow:loss = 0.613483, step = 4901 (47.156 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/tmpcvnt3rp4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.393866.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f1b08afa208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': imgs_train}, y=labels_train_class,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-01-17-12:27:05\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpcvnt3rp4/model.ckpt-5000\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-17-12:27:09\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.527724, global_step = 5000, loss = 1.87391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.52772361, 'global_step': 5000, 'loss': 1.8739115}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': imgs_test}, y=labels_test_class,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpcvnt3rp4/model.ckpt-5000\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the images class\n",
    "preds = list(model.predict(input_fn))\n",
    "\n",
    "# Display\n",
    "#for i in range(n_images):\n",
    "#    plt.imshow(np.reshape(test_images[i], [img_size, img_size]), cmap='gray')\n",
    "#    plt.show()\n",
    "#    print(\"Model prediction:\", class_labels[preds[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cls_pred, cls_true):\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "    \n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_true,\n",
    "                          y_pred=cls_pred)\n",
    "\n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix as an image.\n",
    "    plt.matshow(cm)\n",
    "\n",
    "    # Make various adjustments to the plot.\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, range(num_classes))\n",
    "    plt.yticks(tick_marks, range(num_classes))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[207   5  58  41  84  23  73]\n",
      " [ 13  22   8   3   4   1   4]\n",
      " [ 75   1 177  32 112  55  76]\n",
      " [ 35   3  28 672  61  27  53]\n",
      " [ 74   5  97  69 213  15 121]\n",
      " [  9   2  48  24  20 285  28]\n",
      " [ 48   2  61  67 106  24 318]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAD0CAYAAABuOhhTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhcAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ae1cd5390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cls_pred=preds, cls_true=labels_test_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
